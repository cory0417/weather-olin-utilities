---
jupyter: python3
---

# Utility Bill through the lens of Climate

---
Author: Daeyoung Kim, Ellen Sun \
Date: 2023-03-36

## Introduction

For our Software Design midterm project, we decided to analyze the patterns in the giant excel sheet of utility bill of Olin through the lens of our local climate. The excel sheet contained the electricity bill from FY01 to November of FY23 with variety of information like date, total electricity consumption, time of peak demand, supply cost, demand-response, and many more. 
<p align="center">
    <img src="images/excel_sheet.png" height="500" alt="Olin's electricity bill excel sheet" />
</p>

Our central research question was:
>How does the variation in Boston climate affect Olin College’s electricity consumption and costs?

## Tidying the Utility Data

To make our dataset useful for analysis, we need to first *tidy* the data. 

Currently, the spreadsheet has rows as variables and columns as observation. The observations are also spread across different tabs by their fiscal year. There are empty rows in the sheet as well as non-uniform nomenclature. The format of values are not standarized and vary across cells within the rows. Thus, the data right now is not very useful to us.  

There are 3 rules to tidy data:
>1. Every column is a variable.
>2. Every row is an observation.
>3. Every cell is a single value.

[Source](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)

We processed the dataset to adhere to the 3 rules for convenient analysis. 

Below is the code to autoreload cells for our own convenience.

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import pandas as pd

# Read the excel sheet and store them as a list of dataframes
df_electric_all = []
excel_electricity = pd.ExcelFile(
    "data/FY23_Electric_Data_Campus_Houses_edited.xlsx"
)
for i in range(13, 24):
    df_electric = pd.read_excel(
        excel_electricity, f"FY{i}", header=None, usecols="C:O", nrows=44
    )
    df_electric_transpose = df_electric.T
    valid_cols_mask = df_electric_transpose.iloc[0, :].notnull()
    df_electric_select = df_electric_transpose.loc[:, valid_cols_mask]
    df_electric_all.append(df_electric_select)


# Select relevant data (columns) for the scope of the project
df_electric_filtered = []
for i in range(0, 11):
    useful_cols = [
        "Start Read Date",
        "End Read Date",
        "Time of Peak Demand",
        "Total Cons. (kwh)",
        "Total Monthly Electricity Cost",
    ]
    df_electric_filtered.append(
        df_electric_all[i].loc[
            :, df_electric_all[i].iloc[0, :].isin(useful_cols)
        ]
    )
    df_electric_filtered[i].columns = [
        "start_read_date",
        "end_read_date",
        "total_consumption",
        "time_of_peak_demand",
        "total_cost",
    ]
    df_electric_filtered[i] = df_electric_filtered[i].iloc[1:, :]

# Concatenating dataframes into a single long dataframe
df_electric_long = pd.concat(df_electric_filtered, ignore_index=True)

# Removing rows with NaN's
df_electric_long.dropna(axis=0, inplace=True)

# Save dataframe locally as a .csv file
df_electric_long.to_csv("data/electricity_FY13_23.csv", index=False)
```

Here's a sample of how it looked after processing:

```{python}
from IPython.display import HTML

display(HTML(df_electric_long.head(10).to_html()))
```

## Gathering Climate Data
In order to answer our research question, we used the publicly available National Centers for Environmental Information's Climate Data Online (CDO) API to gather Boston's climate data. We chose Boston as our source of 'local' climate since it was geographically proximal to Olin College and had the most comprehensive data because of Boston Logan Airport's stable data collection. 

The employed steps in gathering data from the API were:
1. Find the Massachusetts ID used in the API
2. Find weather stations in MA
3. Choose a station and find available datatypes
4. Choose relevant datatypes and fetch data
5. Save into a .json and pandas dataframe

```{python}
import requests

request_url = "https://www.ncei.noaa.gov/cdo-web/api/v2/locations?locationcategoryid=ST&limit=52"

with open("API_KEY.txt", "r", encoding="utf-8") as file:
    token = file.read()

response = requests.get(request_url, headers={"token": token})
print(response.json()["results"][21])
```

Now, using `FIPS:25` as ID for Massachusetts, we found all the available stations. We found the ID (`GHCND:USW00014739`) for the Boston Logan weather station, which was the station that had the most coverage and would work best for our needs.

```{python}
import json

request_url = (
    "https://www.ncei.noaa.gov/cdo-web/api/v2/stations/GHCND:USW00014739"
)

response = requests.get(request_url, headers={"token": token})
boston_logan_airport = json.dumps(response.json(), indent=4)
print(boston_logan_airport)
```

```{python}
import pandas as pd

START_DATE = "2013-04-01"
END_DATE = "2022-12-31"
request_url = "https://www.ncei.noaa.gov/cdo-web/api/v2/datatypes?stationid=GHCND:USW00014739&startdate=2013-04-01&enddate=2022-12-31"
response = requests.get(request_url, headers={"token": token})
df_datatypes = pd.json_normalize(response.json(), record_path=["results"])
```

Here were some of the available datatypes:

```{python}
display(HTML(df_datatypes.head(10).to_html()))
```

Although our intial approach was using the API for listing the available datatypes, the results were unorganized even with start and end date queries and did not give us the information that we needed. Thus, we found an [online documentation page](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00014739/detail) of the specific weather station and looked for available datatypes there. 
<p align="center">
    <img src="images/station.png" height="500" alt="Station documentation page" />
</p>

We chose `TAVG`, `AWND`, and `PRCP` for the datatypes and saved them locally as .json files. 

```{python}
from functions import get_data_api
from os import path

DATATYPES = ["TAVG", "PRCP", "AWND"]

for datatype in DATATYPES:
    if path.isfile(f"data/{datatype}.json"):
        print(f"{datatype} dataset is ready!")
    else:
        get_data_api(datatype)
```

After storing the information about each datatype into a `.json` file, we flattened the `.json` files into `pandas` dataframes. Then, we read the utilities data into a pandas dataframe and created a joint dataframe between each weather datatype and the utility information to make it easier to plot. Below is a sample of how the dataframe containing information about the average wind speed looks like. 

```{python}
import pandas as pd
from functions import flatten_json, join_dataframes

df_avg_temp = flatten_json("TAVG")
df_total_prcp = flatten_json("PRCP")
df_avg_wind = flatten_json("AWND")
df_util = pd.read_csv("./data/electricity_FY13_23.csv")

df_util_wind = join_dataframes(df_avg_wind, df_util)
df_util_temp = join_dataframes(df_avg_temp, df_util)
df_util_prcp = join_dataframes(df_total_prcp, df_util)

display(HTML(df_util_wind.head(10).to_html()))

import os

parent_dir = os.path.abspath(os.path.join(os.getcwd()))
print(parent_dir)
```

The joint dataframes contain columns detailing utility information (`total_consumption`, `time_of_peak_demand`, `total_cost`) and weather information (`datatype`, showing which category this data is from, and `value`, showing the actual value taken-- in the displayed case, the average monthly wind speed in MPH)

Now we can plot each weather datatype against the `total monthly electricity consumption` (kwh) of Olin.  

```{python}
from functions import plot_weather_util_2_plots

plot_weather_util_2_plots(df_util_temp)
plot_weather_util_2_plots(df_util_prcp)
plot_weather_util_2_plots(df_util_wind)
```

From these graphs, we noticed three interesting patterns:
- `Average Temperature` had a `highly positive` correlation with electricity consumption
- `Average Wind Speed` had a `fairly negative` correlation with electricity consumption 
- Olin's electricity consumption seemed to rise significantly around 2016-2017, which has no apparent cause from any of these three weather patterns.
    
Firstly, we hypothesized that temperature might be correlated with electricity consumption because Olin uses more air conditioning in the summer and turns it off in the winter. To check this, we created box plots to see the consumption for each of the seasons. The seasons were sorted by month, with December - Feburary being winter, March - May being spring, June - August being summer, and September - November being fall.

```{python}
import matplotlib.pyplot as plt
from functions import filter_season

SEASON_NAMES = ["Winter", "Spring", "Summer", "Fall"]

season_dfs = []
for season in SEASON_NAMES:
    df = filter_season(season, df_util_temp)
    season_dfs.append(df.reset_index())

season_data = []
for df in season_dfs:
    season_data.append(df["total_consumption"])

plt.boxplot(season_data, labels=SEASON_NAMES)
plt.ylabel("Total Energy Consumption (kwh)")
plt.title("Total Energy Consumption in each Season (kwh)")
plt.show()
```

The boxplot above shows the ranges and medians of electricity consumption for each of the seasons. As we expected, winter had significantly lower consumption while summer had significantly higher values. This information supports our hypothesis that the high correlation between temperature and electricity consumption was indeed because of how Olin manages its power usage during the winter and summer seasons. 

While the connection between temperature and electricity consumption might be intuitive, the correlation found for average wind speed is not. To explore more into this phenomenon, we graphed `wind speed` against `average temperature`.

```{python}
import matplotlib.pyplot as plt
import numpy as np

avg_temp = df_util_temp["value"]
avg_wind = df_util_wind["value"]
plt.scatter(avg_temp, avg_wind, color="blue")
plt.xlabel("Average Temperature (F°)")
plt.ylabel("Average Wind Speed (mph)")
plt.title("Average Wind Speed vs Average Temperature")

z = np.polyfit(avg_temp, avg_wind, 1)
p = np.poly1d(z)
plt.plot(avg_temp, p(avg_temp), color="red", label="Trendline")
plt.legend()
plt.show()
```

Looking at the graph above, we can see that average wind speed is clearly `negatively correlated` with average temperature, meaning the correlation between wind speed and electricity consumption was mostly likely rooted in temperature differences instead of much to do with wind speed itself.

Lastly, we investigated the rise in consumption in 2016-2017 by looking at enrollment numbers online. There was about a 5.7% increase in enrollment (20 people), but that isn't likely to cause such a significant increase in consumption. 

## Which features are the most important?
We were also curious to see which features were most relevant in our target variable, `Total Cons. (kwh)`. So, we merged all the data we had available -- climate and utility -- to create a single dataframe for analysis. 

```{python}
from functions import merge_all_df

df_all_data = merge_all_df([df_avg_temp, df_avg_wind, df_total_prcp])
```

Here's what the dataframe looked like after the processing:

```{python}
display(HTML(df_all_data.head(10).to_html()))
```

To analyze which features were the most influential predictors of the total consumption of electricity, we employed the analysis of variance (ANOVA) F-test.

To give a little bit of context, the ANOVA F-test statistic is defined as 
- the ratio of mean square error to regression mean square
    - **mean square error**: the mean of squares of the error between the observed and the predicted value (in our case, generated using linear regression model)
    - **regression mean square**: the mean of squares of difference between the fitted value and the mean of observed values. 

Or alternatively, as
- the ratio of **variation between groups** to **variation within the group**

Another important statistic we obtained was the p-value. The **p-value** is an indicator that if below a certain threshold, ⍺, the result of the statistic is significant. A common value for ⍺ is 0.05. 

A higher F-statistic, combined with a p-value below ⍺, could be interpreted as having greater influence on the target variable.
In our case, the target variable was the total monthly electricity consumption and the features were: 
- monthly total precipitation
- month of year
- time of peak demand
- monthly average wind speed
- monthly average temperature
- total monthly electricity cost

```{python}
from functions import test_features

## Separating the features and the target
features = df_all_data.drop(columns=["total_consumption", "year-month"])
target = df_all_data["total_consumption"]

df_results = test_features(features, target)
```

Here are the results:

```{python}
display(HTML(df_results.to_html()))
```

To visualize the results, we made a bar plot that shows the F-Scores of different features. 

```{python}
from functions import plot_f_test

plot_f_test(df_results)
```

