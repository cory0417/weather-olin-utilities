---
jupyter: python3
---

# Utility Bill through the lens of Climate

---
Author: Daeyoung Kim, Ellen Sun \
Date: 2023-03-36

## Introduction

For our Software Design midterm project, we decided to analyze the patterns in the giant excel sheet of utility bill of Olin through the lens of our local climate. The excel sheet contained the electricity bill from FY01 to November of FY23 with variety of information like date, total electricity consumption, time of peak demand, supply cost, demand-response, and many more. 
<p align="center">
    <img src="images/excel_sheet.png" height="500" alt="Olin's electricity bill excel sheet" />
</p>

Our central research question was:
>How does the variation in Boston climate affect Olin College’s electricity consumption and costs?

## Tidying the Utility Data

To make our dataset useful for analysis, we need to first *tidy* the data. 

Currently, the spreadsheet has rows as variables and columns as observation. The observations are also spread across different tabs by their fiscal year. There are empty rows in the sheet as well as non-uniform nomenclature. The format of values are not standarized and vary across cells within the rows. Thus, the data right now is not very useful to us.  

There are 3 rules to tidy data:
>1. Every column is a variable.
>2. Every row is an observation.
>3. Every cell is a single value.

[Source](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)

We processed the dataset to adhere to the 3 rules for convenient analysis. 

```{python}
%load_ext autoreload
%autoreload 2

import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, f_regression
import json
from os import path
import pandas as pd
import functions
import requests
from IPython.display import HTML
from plotting import df_avg_temp, df_avg_wind, df_total_precp

plt.rcParams["font.family"] = "Helvetica"
%matplotlib inline
```

```{python}
if path.isfile("data/electricity_FY13_23.csv"):
    print("The utility data is ready!")
else:
    functions.tidy_data("data/FY23_Electric_Data_Campus_Houses_edited.xlsx")
```

Here's a sample of how it looked after processing:

```{python}
df_electric_long = pd.read_csv("data/electricity_FY13_23.csv")
display(HTML(df_electric_long.head(10).to_html()))
```


## Gathering Climate Data
In order to answer our research question, we used the publicly available National Centers for Environmental Information's Climate Data Online (CDO) API to gather Boston's climate data. We chose Boston as our source of 'local' climate since it was geographically proximal to Olin College and had the most comprehensive data because of Boston Logan Airport's stable data collection. 

The employed steps in gathering data from the API were:
1. Find the Massachusetts ID used in the API
2. Find weather stations in MA
3. Choose a station and find available datatypes
4. Choose relevant datatypes and fetch data
5. Save into a .json and pandas dataframe

```{python}
request_url = (
    "https://www.ncei.noaa.gov/cdo-web/api/v2/locations?locationcategoryid=ST&limit=52"
)

with open("API_KEY.txt", "r", encoding="utf-8") as file:
    token = file.read()

response = requests.get(request_url, headers={"token": token})
print(response.json()["results"][21])
```

Now, using `FIPS:25` as ID for Massachusetts, we found all the available stations. We found the ID (`GHCND:USW00014739`) for the Boston Logan weather station, which was the station that had the most coverage and would work best for our needs.

```{python}
request_url = "https://www.ncei.noaa.gov/cdo-web/api/v2/stations/GHCND:USW00014739"

response = requests.get(request_url, headers={"token": token})
boston_logan_airport = json.dumps(response.json(), indent=4)
print(boston_logan_airport)
```

```{python}
START_DATE = "2013-04-01"
END_DATE = "2022-12-31"
request_url = "https://www.ncei.noaa.gov/cdo-web/api/v2/datatypes?stationid=GHCND:USW00014739&startdate=2013-04-01&enddate=2022-12-31"
response = requests.get(request_url, headers={"token": token})
df_datatypes = pd.json_normalize(response.json(), record_path=["results"])
```

Here were some of the available datatypes:

```{python}
display(HTML(df_datatypes.head(10).to_html()))
```

Although our intial approach was using the API for listing the available datatypes, the results were unorganized even with start and end date queries and did not give us the information that we needed. Thus, we found an [online documentation page](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00014739/detail) of the specific weather station and looked for available datatypes there. 
<p align="center">
    <img src="images/station.png" height="500" alt="Station documentation page" />
</p>

We chose `TAVG`, `AWND`, and `PRCP` for the datatypes and saved them locally as .json files. 

```{python}
DATATYPES = ["TAVG", "PRCP", "AWND"]

for datatype in DATATYPES:
    if path.isfile(f"data/{datatype}.json"):
        print(f"{datatype} dataset is ready!")
    else:
        functions.get_data_api(datatype)
```

## Which features are the most important?
We were also curious to see which features were most relevant in our target variable, `Total Cons. (kwh)`. So, we merged all the data we had available -- climate and utility -- to create a single dataframe for analysis. 

```{python}
df_all_data = functions.merge_all_df([df_avg_temp, df_avg_wind, df_total_precp])
```

Here's what the dataframe looked like after the processing:

```{python}
display(HTML(df_all_data.head(10).to_html()))
```

To analyze which features were the most influential predictors of the total consumption of electricity, we employed the analysis of variance (ANOVA) F-test.

To give a little bit of context, the ANOVA F-test statistic is defined as 
- the ratio of mean square error to regression mean square
    - **mean square error**: the mean of squares of the error between the observed and the predicted value (in our case, generated using linear regression model)
    - **regression mean square**: the mean of squares of difference between the fitted value and the mean of observed values. 

Or alternatively, as
- the ratio of **variation between groups** to **variation within the group**

Another important statistic we obtained was the p-value. The **p-value** is an indicator that if below a certain threshold, ⍺, the result of the statistic is significant. A common value for ⍺ is 0.05. 

A higher F-statistic, combined with a p-value below ⍺, could be interpreted as having greater influence on the target variable.
In our case, the target variable was the total monthly electricity consumption and the features were: 
- monthly total precipitation
- month of year
- time of peak demand
- monthly average wind speed
- monthly average temperature
- total monthly electricity cost


```{python}
## Separating the features and the target
features = df_all_data.drop(columns=["total_consumption", "year-month"])
target = df_all_data["total_consumption"]

df_results = functions.test_features(features, target)
```

```{python}

# Create an instance of SelectKBest using the f_regression method
selector_k_best = SelectKBest(score_func=f_regression, k="all")
features_new = selector_k_best.fit_transform(features, target)
```

Here are the results:

```{python}

display(HTML(df_results.to_html()))


### TEST p-value > 0, F-score > 0
```


To visualize the results, we made a bar plot that shows the F-Scores of different features. 

```{python}
results_df = results_df.sort_values("F-Score", ascending=True)
ylabels = [
    "Monthly Total\n Precipitation",
    "Month of\n Year",
    "Time of\n Peak Demand",
    "Monthly Average\n Wind Speed",
    "Monthly Average\n Temperature",
    "Total Monthly\n Electricity Cost",
]

fig, ax = plt.subplots()
ax.barh(ylabels, results_df["F-Score"])
ax.set_ylabel("Features")
ax.set_xlabel("F-Score")
ax.set_title("F-Score of Features for \nTotal Monthly Electricity Consumption")
plt.show()
```

